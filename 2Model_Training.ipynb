{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.dat', mode='rb'))\n",
    "\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the Book scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    # Get number of batches\n",
    "    num_batches = len(words) // batch_size\n",
    "    # Only consider words that would fit into full batches\n",
    "    trimmed_words = words[:num_batches * batch_size]\n",
    "    # Determine last index to have full sequences\n",
    "    final_idx = len(trimmed_words) - sequence_length\n",
    "    x, y = [], []\n",
    "    for idx in range(final_idx):\n",
    "        idx_end = idx + sequence_length\n",
    "        x_batch = trimmed_words[idx:idx_end]\n",
    "        x.append(x_batch)\n",
    "        y_batch = trimmed_words[idx_end]\n",
    "        y.append(y_batch)\n",
    "    \n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
    "    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def save_model(filename, decoder):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    return torch.load(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # set class variables\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define model layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        embed = self.embedding(nn_input.long())\n",
    "        lstm_out, hidden = self.lstm(embed, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.fc(lstm_out)\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1]\n",
    "\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param rnn: The PyTorch Module that holds the neural network\n",
    "    :param optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    # move data to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        rnn.cuda()\n",
    "    \n",
    "    # perform backpropagation and optimization\n",
    "    h = tuple([h_tmp.data for h_tmp in hidden])\n",
    "    \n",
    "    # Zero out gradients\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    # Copy to GPU\n",
    "    if(train_on_gpu):\n",
    "        inp, target = inp.cuda(), target.type(torch.LongTensor).cuda()\n",
    "        \n",
    "    # Predict output\n",
    "    output, h = rnn(inp, h)\n",
    "    \n",
    "    # Calc loss\n",
    "    # loss = criterion(output, target)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Prevent exploding gradient with RNN/LSTM\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 4)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    losses = []\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        print(f\"Epoch {epoch_i}\")\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            # print(f\"Epoch {epoch_i} | Batch {batch_i}\")\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print(f'\\tEpoch: {epoch_i:>4}/{n_epochs:<4}  Loss: {np.average(batch_losses)}')\n",
    "                losses.append(np.average(batch_losses))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [14, 15, 16, 17, 18],\n",
      "        [28, 29, 30, 31, 32],\n",
      "        [44, 45, 46, 47, 48],\n",
      "        [17, 18, 19, 20, 21],\n",
      "        [19, 20, 21, 22, 23],\n",
      "        [36, 37, 38, 39, 40],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [33, 34, 35, 36, 37],\n",
      "        [13, 14, 15, 16, 17]], dtype=torch.int32)\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([ 6, 19, 33, 49, 22, 24, 41, 20, 38, 18], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 20  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 50\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim = 200\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50 epoch(s)...\n",
      "Epoch 1\n",
      "\tEpoch:    1/50    Loss: 6.692838034629822\n",
      "\tEpoch:    1/50    Loss: 6.269230317115784\n",
      "\tEpoch:    1/50    Loss: 6.04715196800232\n",
      "\tEpoch:    1/50    Loss: 5.9450034379959105\n",
      "\tEpoch:    1/50    Loss: 5.865376475334167\n",
      "\tEpoch:    1/50    Loss: 5.789781674385071\n",
      "\tEpoch:    1/50    Loss: 5.7467555913925175\n",
      "\tEpoch:    1/50    Loss: 5.710786415100098\n",
      "\tEpoch:    1/50    Loss: 5.662838299751281\n",
      "\tEpoch:    1/50    Loss: 5.616340503692627\n",
      "\tEpoch:    1/50    Loss: 5.599313571929931\n",
      "\tEpoch:    1/50    Loss: 5.560633892059326\n",
      "Epoch 2\n",
      "\tEpoch:    2/50    Loss: 5.442950360954306\n",
      "\tEpoch:    2/50    Loss: 5.3926104078292845\n",
      "\tEpoch:    2/50    Loss: 5.375588607788086\n",
      "\tEpoch:    2/50    Loss: 5.366182539939881\n",
      "\tEpoch:    2/50    Loss: 5.353274345397949\n",
      "\tEpoch:    2/50    Loss: 5.338656592369079\n",
      "\tEpoch:    2/50    Loss: 5.337219166755676\n",
      "\tEpoch:    2/50    Loss: 5.3216510543823246\n",
      "\tEpoch:    2/50    Loss: 5.319103585243225\n",
      "\tEpoch:    2/50    Loss: 5.2957671995162965\n",
      "\tEpoch:    2/50    Loss: 5.312605226516724\n",
      "\tEpoch:    2/50    Loss: 5.288958930969239\n",
      "Epoch 3\n",
      "\tEpoch:    3/50    Loss: 5.149936900497479\n",
      "\tEpoch:    3/50    Loss: 5.089296964645386\n",
      "\tEpoch:    3/50    Loss: 5.1087033081054685\n",
      "\tEpoch:    3/50    Loss: 5.11371427154541\n",
      "\tEpoch:    3/50    Loss: 5.090654006004334\n",
      "\tEpoch:    3/50    Loss: 5.071927344322204\n",
      "\tEpoch:    3/50    Loss: 5.105989848136902\n",
      "\tEpoch:    3/50    Loss: 5.074956003189087\n",
      "\tEpoch:    3/50    Loss: 5.0985199022293095\n",
      "\tEpoch:    3/50    Loss: 5.095652060508728\n",
      "\tEpoch:    3/50    Loss: 5.120328059196472\n",
      "\tEpoch:    3/50    Loss: 5.091036345481872\n",
      "Epoch 4\n",
      "\tEpoch:    4/50    Loss: 4.953421987198854\n",
      "\tEpoch:    4/50    Loss: 4.882620409011841\n",
      "\tEpoch:    4/50    Loss: 4.8905073156356815\n",
      "\tEpoch:    4/50    Loss: 4.909365839958191\n",
      "\tEpoch:    4/50    Loss: 4.922265276908875\n",
      "\tEpoch:    4/50    Loss: 4.925879726409912\n",
      "\tEpoch:    4/50    Loss: 4.947927054405213\n",
      "\tEpoch:    4/50    Loss: 4.9411807556152345\n",
      "\tEpoch:    4/50    Loss: 4.937944836616516\n",
      "\tEpoch:    4/50    Loss: 4.952772307395935\n",
      "\tEpoch:    4/50    Loss: 4.954926170349121\n",
      "\tEpoch:    4/50    Loss: 4.970425682067871\n",
      "Epoch 5\n",
      "\tEpoch:    5/50    Loss: 4.800711655019054\n",
      "\tEpoch:    5/50    Loss: 4.739651387214661\n",
      "\tEpoch:    5/50    Loss: 4.755441158294678\n",
      "\tEpoch:    5/50    Loss: 4.7654417057037355\n",
      "\tEpoch:    5/50    Loss: 4.778972101211548\n",
      "\tEpoch:    5/50    Loss: 4.801327466011047\n",
      "\tEpoch:    5/50    Loss: 4.8034876976013186\n",
      "\tEpoch:    5/50    Loss: 4.8139897050857545\n",
      "\tEpoch:    5/50    Loss: 4.833887961387634\n",
      "\tEpoch:    5/50    Loss: 4.82290215587616\n",
      "\tEpoch:    5/50    Loss: 4.8392168884277345\n",
      "\tEpoch:    5/50    Loss: 4.839475317955017\n",
      "Epoch 6\n",
      "\tEpoch:    6/50    Loss: 4.6961902011403795\n",
      "\tEpoch:    6/50    Loss: 4.649818161964417\n",
      "\tEpoch:    6/50    Loss: 4.645123345375061\n",
      "\tEpoch:    6/50    Loss: 4.659422927856445\n",
      "\tEpoch:    6/50    Loss: 4.678080900192261\n",
      "\tEpoch:    6/50    Loss: 4.673666165351868\n",
      "\tEpoch:    6/50    Loss: 4.70106027507782\n",
      "\tEpoch:    6/50    Loss: 4.702997169017792\n",
      "\tEpoch:    6/50    Loss: 4.732690976142884\n",
      "\tEpoch:    6/50    Loss: 4.73324879693985\n",
      "\tEpoch:    6/50    Loss: 4.747263292312622\n",
      "\tEpoch:    6/50    Loss: 4.755866702079773\n",
      "Epoch 7\n",
      "\tEpoch:    7/50    Loss: 4.579246301504895\n",
      "\tEpoch:    7/50    Loss: 4.527509147167206\n",
      "\tEpoch:    7/50    Loss: 4.5594025993347165\n",
      "\tEpoch:    7/50    Loss: 4.569587592601776\n",
      "\tEpoch:    7/50    Loss: 4.59259336566925\n",
      "\tEpoch:    7/50    Loss: 4.62212061882019\n",
      "\tEpoch:    7/50    Loss: 4.627693878173828\n",
      "\tEpoch:    7/50    Loss: 4.6137266464233395\n",
      "\tEpoch:    7/50    Loss: 4.643569181442261\n",
      "\tEpoch:    7/50    Loss: 4.643501806259155\n",
      "\tEpoch:    7/50    Loss: 4.6765329051017765\n",
      "\tEpoch:    7/50    Loss: 4.698464139938355\n",
      "Epoch 8\n",
      "\tEpoch:    8/50    Loss: 4.514318058749759\n",
      "\tEpoch:    8/50    Loss: 4.453364815711975\n",
      "\tEpoch:    8/50    Loss: 4.494516921520233\n",
      "\tEpoch:    8/50    Loss: 4.498550820350647\n",
      "\tEpoch:    8/50    Loss: 4.52472775888443\n",
      "\tEpoch:    8/50    Loss: 4.536590839862823\n",
      "\tEpoch:    8/50    Loss: 4.554825400352478\n",
      "\tEpoch:    8/50    Loss: 4.566719980716705\n",
      "\tEpoch:    8/50    Loss: 4.567934556007385\n",
      "\tEpoch:    8/50    Loss: 4.591736893177033\n",
      "\tEpoch:    8/50    Loss: 4.58929562664032\n",
      "\tEpoch:    8/50    Loss: 4.618813066482544\n",
      "Epoch 9\n",
      "\tEpoch:    9/50    Loss: 4.437043311868205\n",
      "\tEpoch:    9/50    Loss: 4.398751958847046\n",
      "\tEpoch:    9/50    Loss: 4.422618175029755\n",
      "\tEpoch:    9/50    Loss: 4.433118116378784\n",
      "\tEpoch:    9/50    Loss: 4.451689161300659\n",
      "\tEpoch:    9/50    Loss: 4.47027338218689\n",
      "\tEpoch:    9/50    Loss: 4.494223487377167\n",
      "\tEpoch:    9/50    Loss: 4.504052901744843\n",
      "\tEpoch:    9/50    Loss: 4.5042237367630005\n",
      "\tEpoch:    9/50    Loss: 4.52404723072052\n",
      "\tEpoch:    9/50    Loss: 4.551070468902588\n",
      "\tEpoch:    9/50    Loss: 4.568425882816315\n",
      "Epoch 10\n",
      "\tEpoch:   10/50    Loss: 4.397036449822875\n",
      "\tEpoch:   10/50    Loss: 4.340759382247925\n",
      "\tEpoch:   10/50    Loss: 4.35898705291748\n",
      "\tEpoch:   10/50    Loss: 4.378121027946472\n",
      "\tEpoch:   10/50    Loss: 4.40679154253006\n",
      "\tEpoch:   10/50    Loss: 4.421045900821686\n",
      "\tEpoch:   10/50    Loss: 4.433554554462433\n",
      "\tEpoch:   10/50    Loss: 4.436203392982483\n",
      "\tEpoch:   10/50    Loss: 4.460925999641418\n",
      "\tEpoch:   10/50    Loss: 4.4795240540504455\n",
      "\tEpoch:   10/50    Loss: 4.483028873443604\n",
      "\tEpoch:   10/50    Loss: 4.507566303730011\n",
      "Epoch 11\n",
      "\tEpoch:   11/50    Loss: 4.3428152859377\n",
      "\tEpoch:   11/50    Loss: 4.301932154178619\n",
      "\tEpoch:   11/50    Loss: 4.315715669631958\n",
      "\tEpoch:   11/50    Loss: 4.320978605270386\n",
      "\tEpoch:   11/50    Loss: 4.354918200016022\n",
      "\tEpoch:   11/50    Loss: 4.367049414157868\n",
      "\tEpoch:   11/50    Loss: 4.378203030109406\n",
      "\tEpoch:   11/50    Loss: 4.41600031375885\n",
      "\tEpoch:   11/50    Loss: 4.427662951469421\n",
      "\tEpoch:   11/50    Loss: 4.433598567962647\n",
      "\tEpoch:   11/50    Loss: 4.443403385162354\n",
      "\tEpoch:   11/50    Loss: 4.46135009765625\n",
      "Epoch 12\n",
      "\tEpoch:   12/50    Loss: 4.303206951837354\n",
      "\tEpoch:   12/50    Loss: 4.2378833661079405\n",
      "\tEpoch:   12/50    Loss: 4.270255408287048\n",
      "\tEpoch:   12/50    Loss: 4.292994682788849\n",
      "\tEpoch:   12/50    Loss: 4.317734191894531\n",
      "\tEpoch:   12/50    Loss: 4.316127275466919\n",
      "\tEpoch:   12/50    Loss: 4.348272658348083\n",
      "\tEpoch:   12/50    Loss: 4.353356615066528\n",
      "\tEpoch:   12/50    Loss: 4.362457432746887\n",
      "\tEpoch:   12/50    Loss: 4.3939107546806335\n",
      "\tEpoch:   12/50    Loss: 4.415457713603973\n",
      "\tEpoch:   12/50    Loss: 4.409842669010162\n",
      "Epoch 13\n",
      "\tEpoch:   13/50    Loss: 4.253408805573551\n",
      "\tEpoch:   13/50    Loss: 4.194796409130096\n",
      "\tEpoch:   13/50    Loss: 4.219342294216156\n",
      "\tEpoch:   13/50    Loss: 4.263741914272308\n",
      "\tEpoch:   13/50    Loss: 4.274164787769317\n",
      "\tEpoch:   13/50    Loss: 4.31175753736496\n",
      "\tEpoch:   13/50    Loss: 4.312004091739655\n",
      "\tEpoch:   13/50    Loss: 4.339984191894532\n",
      "\tEpoch:   13/50    Loss: 4.330867959022522\n",
      "\tEpoch:   13/50    Loss: 4.358283862113953\n",
      "\tEpoch:   13/50    Loss: 4.362579530239105\n",
      "\tEpoch:   13/50    Loss: 4.393559132099152\n",
      "Epoch 14\n",
      "\tEpoch:   14/50    Loss: 4.2066275489031435\n",
      "\tEpoch:   14/50    Loss: 4.162030869960785\n",
      "\tEpoch:   14/50    Loss: 4.2093282675743104\n",
      "\tEpoch:   14/50    Loss: 4.216417935848236\n",
      "\tEpoch:   14/50    Loss: 4.240695322036744\n",
      "\tEpoch:   14/50    Loss: 4.260238413333893\n",
      "\tEpoch:   14/50    Loss: 4.271928318500519\n",
      "\tEpoch:   14/50    Loss: 4.301732067584991\n",
      "\tEpoch:   14/50    Loss: 4.31588872385025\n",
      "\tEpoch:   14/50    Loss: 4.328000162124634\n",
      "\tEpoch:   14/50    Loss: 4.345748886108399\n",
      "\tEpoch:   14/50    Loss: 4.35049243593216\n",
      "Epoch 15\n",
      "\tEpoch:   15/50    Loss: 4.176853971561018\n",
      "\tEpoch:   15/50    Loss: 4.147859789848328\n",
      "\tEpoch:   15/50    Loss: 4.185222198486328\n",
      "\tEpoch:   15/50    Loss: 4.175453243255615\n",
      "\tEpoch:   15/50    Loss: 4.199084633350372\n",
      "\tEpoch:   15/50    Loss: 4.222803548336029\n",
      "\tEpoch:   15/50    Loss: 4.250209046840668\n",
      "\tEpoch:   15/50    Loss: 4.254615864753723\n",
      "\tEpoch:   15/50    Loss: 4.260362286090851\n",
      "\tEpoch:   15/50    Loss: 4.301115572929382\n",
      "\tEpoch:   15/50    Loss: 4.319997641563416\n",
      "\tEpoch:   15/50    Loss: 4.326775923728943\n",
      "Epoch 16\n",
      "\tEpoch:   16/50    Loss: 4.170171395317758\n",
      "\tEpoch:   16/50    Loss: 4.107221478462219\n",
      "\tEpoch:   16/50    Loss: 4.120893119335174\n",
      "\tEpoch:   16/50    Loss: 4.1662210640907285\n",
      "\tEpoch:   16/50    Loss: 4.172619395732879\n",
      "\tEpoch:   16/50    Loss: 4.18966166639328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch:   16/50    Loss: 4.228574073314666\n",
      "\tEpoch:   16/50    Loss: 4.255858575820922\n",
      "\tEpoch:   16/50    Loss: 4.259651049137116\n",
      "\tEpoch:   16/50    Loss: 4.264527567863464\n",
      "\tEpoch:   16/50    Loss: 4.273568674564362\n",
      "\tEpoch:   16/50    Loss: 4.301086458683014\n",
      "Epoch 17\n",
      "\tEpoch:   17/50    Loss: 4.126284969550321\n",
      "\tEpoch:   17/50    Loss: 4.1053393983840945\n",
      "\tEpoch:   17/50    Loss: 4.103457525730133\n",
      "\tEpoch:   17/50    Loss: 4.128394444942474\n",
      "\tEpoch:   17/50    Loss: 4.158969497680664\n",
      "\tEpoch:   17/50    Loss: 4.165053059577942\n",
      "\tEpoch:   17/50    Loss: 4.194871358394622\n",
      "\tEpoch:   17/50    Loss: 4.19663843870163\n",
      "\tEpoch:   17/50    Loss: 4.227120222091675\n",
      "\tEpoch:   17/50    Loss: 4.2468961906433105\n",
      "\tEpoch:   17/50    Loss: 4.248178778648376\n",
      "\tEpoch:   17/50    Loss: 4.266205548286438\n",
      "Epoch 18\n",
      "\tEpoch:   18/50    Loss: 4.107194478465322\n",
      "\tEpoch:   18/50    Loss: 4.048713207244873\n",
      "\tEpoch:   18/50    Loss: 4.084457123756409\n",
      "\tEpoch:   18/50    Loss: 4.110441961765289\n",
      "\tEpoch:   18/50    Loss: 4.126045877456665\n",
      "\tEpoch:   18/50    Loss: 4.150003106117248\n",
      "\tEpoch:   18/50    Loss: 4.162390106678009\n",
      "\tEpoch:   18/50    Loss: 4.174733794212341\n",
      "\tEpoch:   18/50    Loss: 4.191730453014374\n",
      "\tEpoch:   18/50    Loss: 4.214573786258698\n",
      "\tEpoch:   18/50    Loss: 4.224028658390045\n",
      "\tEpoch:   18/50    Loss: 4.2572696533203125\n",
      "Epoch 19\n",
      "\tEpoch:   19/50    Loss: 4.091454792819647\n",
      "\tEpoch:   19/50    Loss: 4.061451813220978\n",
      "\tEpoch:   19/50    Loss: 4.056749258518219\n",
      "\tEpoch:   19/50    Loss: 4.093768791675568\n",
      "\tEpoch:   19/50    Loss: 4.104068165779114\n",
      "\tEpoch:   19/50    Loss: 4.128301659584046\n",
      "\tEpoch:   19/50    Loss: 4.146894917011261\n",
      "\tEpoch:   19/50    Loss: 4.1516903529167175\n",
      "\tEpoch:   19/50    Loss: 4.172814044952393\n",
      "\tEpoch:   19/50    Loss: 4.184355041980743\n",
      "\tEpoch:   19/50    Loss: 4.207651388168335\n",
      "\tEpoch:   19/50    Loss: 4.221951232910156\n",
      "Epoch 20\n",
      "\tEpoch:   20/50    Loss: 4.0657119860556135\n",
      "\tEpoch:   20/50    Loss: 4.001897636890411\n",
      "\tEpoch:   20/50    Loss: 4.044319493770599\n",
      "\tEpoch:   20/50    Loss: 4.056536514759063\n",
      "\tEpoch:   20/50    Loss: 4.093979051589966\n",
      "\tEpoch:   20/50    Loss: 4.115235980510712\n",
      "\tEpoch:   20/50    Loss: 4.1053421459198\n",
      "\tEpoch:   20/50    Loss: 4.143690801620483\n",
      "\tEpoch:   20/50    Loss: 4.154291098117828\n",
      "\tEpoch:   20/50    Loss: 4.180325860500336\n",
      "\tEpoch:   20/50    Loss: 4.181580726623535\n",
      "\tEpoch:   20/50    Loss: 4.192759119033814\n",
      "Epoch 21\n",
      "\tEpoch:   21/50    Loss: 4.045966887872531\n",
      "\tEpoch:   21/50    Loss: 4.003904952526092\n",
      "\tEpoch:   21/50    Loss: 4.022316232681274\n",
      "\tEpoch:   21/50    Loss: 4.037465635299682\n",
      "\tEpoch:   21/50    Loss: 4.057091356754303\n",
      "\tEpoch:   21/50    Loss: 4.092738174915314\n",
      "\tEpoch:   21/50    Loss: 4.106233288764954\n",
      "\tEpoch:   21/50    Loss: 4.115537932872773\n",
      "\tEpoch:   21/50    Loss: 4.12943580198288\n",
      "\tEpoch:   21/50    Loss: 4.147403347492218\n",
      "\tEpoch:   21/50    Loss: 4.164428113937378\n",
      "\tEpoch:   21/50    Loss: 4.165267259120941\n",
      "Epoch 22\n",
      "\tEpoch:   22/50    Loss: 4.022115305273646\n",
      "\tEpoch:   22/50    Loss: 3.979182450294495\n",
      "\tEpoch:   22/50    Loss: 4.020474544525147\n",
      "\tEpoch:   22/50    Loss: 4.01723760843277\n",
      "\tEpoch:   22/50    Loss: 4.033812854766846\n",
      "\tEpoch:   22/50    Loss: 4.065739061832428\n",
      "\tEpoch:   22/50    Loss: 4.082385168552399\n",
      "\tEpoch:   22/50    Loss: 4.100896520137787\n",
      "\tEpoch:   22/50    Loss: 4.118034102916718\n",
      "\tEpoch:   22/50    Loss: 4.131658378601074\n",
      "\tEpoch:   22/50    Loss: 4.156004379749298\n",
      "\tEpoch:   22/50    Loss: 4.162012134075165\n",
      "Epoch 23\n",
      "\tEpoch:   23/50    Loss: 3.988109713145285\n",
      "\tEpoch:   23/50    Loss: 3.9555771017074584\n",
      "\tEpoch:   23/50    Loss: 4.0006953997612\n",
      "\tEpoch:   23/50    Loss: 3.9982678365707396\n",
      "\tEpoch:   23/50    Loss: 4.028038987636566\n",
      "\tEpoch:   23/50    Loss: 4.051564311504364\n",
      "\tEpoch:   23/50    Loss: 4.076205803871154\n",
      "\tEpoch:   23/50    Loss: 4.08078822517395\n",
      "\tEpoch:   23/50    Loss: 4.0921288313865665\n",
      "\tEpoch:   23/50    Loss: 4.110328008174896\n",
      "\tEpoch:   23/50    Loss: 4.127195307731628\n",
      "\tEpoch:   23/50    Loss: 4.160682629585266\n",
      "Epoch 24\n",
      "\tEpoch:   24/50    Loss: 3.9685353193442467\n",
      "\tEpoch:   24/50    Loss: 3.9348960475921633\n",
      "\tEpoch:   24/50    Loss: 3.9668834829330444\n",
      "\tEpoch:   24/50    Loss: 3.991147418022156\n",
      "\tEpoch:   24/50    Loss: 4.015155208587647\n",
      "\tEpoch:   24/50    Loss: 4.027891520023346\n",
      "\tEpoch:   24/50    Loss: 4.032002052783966\n",
      "\tEpoch:   24/50    Loss: 4.083036992073059\n",
      "\tEpoch:   24/50    Loss: 4.076210745334626\n",
      "\tEpoch:   24/50    Loss: 4.105941780090332\n",
      "\tEpoch:   24/50    Loss: 4.115531782627106\n",
      "\tEpoch:   24/50    Loss: 4.126271048545838\n",
      "Epoch 25\n",
      "\tEpoch:   25/50    Loss: 3.9715479755135963\n",
      "\tEpoch:   25/50    Loss: 3.929105818271637\n",
      "\tEpoch:   25/50    Loss: 3.9367271184921266\n",
      "\tEpoch:   25/50    Loss: 3.9649335670471193\n",
      "\tEpoch:   25/50    Loss: 3.9994765276908875\n",
      "\tEpoch:   25/50    Loss: 4.011359042167664\n",
      "\tEpoch:   25/50    Loss: 4.029893026351929\n",
      "\tEpoch:   25/50    Loss: 4.055548340320587\n",
      "\tEpoch:   25/50    Loss: 4.05582581949234\n",
      "\tEpoch:   25/50    Loss: 4.09291091299057\n",
      "\tEpoch:   25/50    Loss: 4.102358997821808\n",
      "\tEpoch:   25/50    Loss: 4.115142781734466\n",
      "Epoch 26\n",
      "\tEpoch:   26/50    Loss: 3.944802443961247\n",
      "\tEpoch:   26/50    Loss: 3.911807698726654\n",
      "\tEpoch:   26/50    Loss: 3.933402272224426\n",
      "\tEpoch:   26/50    Loss: 3.9693527851104737\n",
      "\tEpoch:   26/50    Loss: 3.96192400598526\n",
      "\tEpoch:   26/50    Loss: 3.9912044892311096\n",
      "\tEpoch:   26/50    Loss: 4.031306892871856\n",
      "\tEpoch:   26/50    Loss: 4.030375557422638\n",
      "\tEpoch:   26/50    Loss: 4.046190262794495\n",
      "\tEpoch:   26/50    Loss: 4.054281608104706\n",
      "\tEpoch:   26/50    Loss: 4.097782307624817\n",
      "\tEpoch:   26/50    Loss: 4.104789845466613\n",
      "Epoch 27\n",
      "\tEpoch:   27/50    Loss: 3.933276061228059\n",
      "\tEpoch:   27/50    Loss: 3.892822786331177\n",
      "\tEpoch:   27/50    Loss: 3.9231936349868772\n",
      "\tEpoch:   27/50    Loss: 3.938235977649689\n",
      "\tEpoch:   27/50    Loss: 3.97950790309906\n",
      "\tEpoch:   27/50    Loss: 3.9885469613075255\n",
      "\tEpoch:   27/50    Loss: 4.005745367050171\n",
      "\tEpoch:   27/50    Loss: 4.031692286968231\n",
      "\tEpoch:   27/50    Loss: 4.025376424789429\n",
      "\tEpoch:   27/50    Loss: 4.055540960311889\n",
      "\tEpoch:   27/50    Loss: 4.0592432174682616\n",
      "\tEpoch:   27/50    Loss: 4.086732481956482\n",
      "Epoch 28\n",
      "\tEpoch:   28/50    Loss: 3.925962781507657\n",
      "\tEpoch:   28/50    Loss: 3.8894155492782594\n",
      "\tEpoch:   28/50    Loss: 3.912438334941864\n",
      "\tEpoch:   28/50    Loss: 3.9216565117836\n",
      "\tEpoch:   28/50    Loss: 3.9571493349075317\n",
      "\tEpoch:   28/50    Loss: 3.958251976966858\n",
      "\tEpoch:   28/50    Loss: 3.9853448419570925\n",
      "\tEpoch:   28/50    Loss: 3.9925368695259094\n",
      "\tEpoch:   28/50    Loss: 4.036983054637909\n",
      "\tEpoch:   28/50    Loss: 4.046516592502594\n",
      "\tEpoch:   28/50    Loss: 4.034645036697388\n",
      "\tEpoch:   28/50    Loss: 4.05980361032486\n",
      "Epoch 29\n",
      "\tEpoch:   29/50    Loss: 3.8990245928007248\n",
      "\tEpoch:   29/50    Loss: 3.8655940008163454\n",
      "\tEpoch:   29/50    Loss: 3.885931864738464\n",
      "\tEpoch:   29/50    Loss: 3.912809468269348\n",
      "\tEpoch:   29/50    Loss: 3.934308861732483\n",
      "\tEpoch:   29/50    Loss: 3.9609286303520204\n",
      "\tEpoch:   29/50    Loss: 3.978278538227081\n",
      "\tEpoch:   29/50    Loss: 3.9847891373634337\n",
      "\tEpoch:   29/50    Loss: 4.0176268587112425\n",
      "\tEpoch:   29/50    Loss: 4.029180385112762\n",
      "\tEpoch:   29/50    Loss: 4.0382783713340755\n",
      "\tEpoch:   29/50    Loss: 4.060705813407898\n",
      "Epoch 30\n",
      "\tEpoch:   30/50    Loss: 3.908057974905689\n",
      "\tEpoch:   30/50    Loss: 3.84770649433136\n",
      "\tEpoch:   30/50    Loss: 3.874857799053192\n",
      "\tEpoch:   30/50    Loss: 3.89140757226944\n",
      "\tEpoch:   30/50    Loss: 3.9312860107421876\n",
      "\tEpoch:   30/50    Loss: 3.946051603317261\n",
      "\tEpoch:   30/50    Loss: 3.978442271232605\n",
      "\tEpoch:   30/50    Loss: 3.9859692554473876\n",
      "\tEpoch:   30/50    Loss: 3.984926320552826\n",
      "\tEpoch:   30/50    Loss: 4.0157213125228886\n",
      "\tEpoch:   30/50    Loss: 4.024085102081298\n",
      "\tEpoch:   30/50    Loss: 4.057415190696716\n",
      "Epoch 31\n",
      "\tEpoch:   31/50    Loss: 3.8876556443636794\n",
      "\tEpoch:   31/50    Loss: 3.8384220690727235\n",
      "\tEpoch:   31/50    Loss: 3.8573688831329345\n",
      "\tEpoch:   31/50    Loss: 3.9028662662506104\n",
      "\tEpoch:   31/50    Loss: 3.908839741230011\n",
      "\tEpoch:   31/50    Loss: 3.92325176525116\n",
      "\tEpoch:   31/50    Loss: 3.955209666252136\n",
      "\tEpoch:   31/50    Loss: 3.965058947086334\n",
      "\tEpoch:   31/50    Loss: 3.98319820022583\n",
      "\tEpoch:   31/50    Loss: 3.997664381504059\n",
      "\tEpoch:   31/50    Loss: 4.031770931243896\n",
      "\tEpoch:   31/50    Loss: 4.043985266208649\n",
      "Epoch 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch:   32/50    Loss: 3.8696814924893603\n",
      "\tEpoch:   32/50    Loss: 3.842768452167511\n",
      "\tEpoch:   32/50    Loss: 3.85341836977005\n",
      "\tEpoch:   32/50    Loss: 3.8797004890441893\n",
      "\tEpoch:   32/50    Loss: 3.8970921664237976\n",
      "\tEpoch:   32/50    Loss: 3.9182410521507265\n",
      "\tEpoch:   32/50    Loss: 3.9383519921302796\n",
      "\tEpoch:   32/50    Loss: 3.957512380123138\n",
      "\tEpoch:   32/50    Loss: 3.974120273590088\n",
      "\tEpoch:   32/50    Loss: 4.007708135604858\n",
      "\tEpoch:   32/50    Loss: 3.9918698616027832\n",
      "\tEpoch:   32/50    Loss: 4.011325295925141\n",
      "Epoch 33\n",
      "\tEpoch:   33/50    Loss: 3.859448933335732\n",
      "\tEpoch:   33/50    Loss: 3.828968981266022\n",
      "\tEpoch:   33/50    Loss: 3.851298785209656\n",
      "\tEpoch:   33/50    Loss: 3.8810183396339415\n",
      "\tEpoch:   33/50    Loss: 3.8943512268066405\n",
      "\tEpoch:   33/50    Loss: 3.920953032016754\n",
      "\tEpoch:   33/50    Loss: 3.9097440304756166\n",
      "\tEpoch:   33/50    Loss: 3.9437841734886168\n",
      "\tEpoch:   33/50    Loss: 3.952465356826782\n",
      "\tEpoch:   33/50    Loss: 3.969167974472046\n",
      "\tEpoch:   33/50    Loss: 3.991802806377411\n",
      "\tEpoch:   33/50    Loss: 4.01353217458725\n",
      "Epoch 34\n",
      "\tEpoch:   34/50    Loss: 3.8508790107158566\n",
      "\tEpoch:   34/50    Loss: 3.8231453123092654\n",
      "\tEpoch:   34/50    Loss: 3.833953959941864\n",
      "\tEpoch:   34/50    Loss: 3.8501236357688904\n",
      "\tEpoch:   34/50    Loss: 3.865032141685486\n",
      "\tEpoch:   34/50    Loss: 3.896717661857605\n",
      "\tEpoch:   34/50    Loss: 3.9131678528785705\n",
      "\tEpoch:   34/50    Loss: 3.9325066866874696\n",
      "\tEpoch:   34/50    Loss: 3.959912212371826\n",
      "\tEpoch:   34/50    Loss: 3.978762085914612\n",
      "\tEpoch:   34/50    Loss: 3.9800268297195434\n",
      "\tEpoch:   34/50    Loss: 3.9951986179351806\n",
      "Epoch 35\n",
      "\tEpoch:   35/50    Loss: 3.836687954023356\n",
      "\tEpoch:   35/50    Loss: 3.804583403110504\n",
      "\tEpoch:   35/50    Loss: 3.8384375982284547\n",
      "\tEpoch:   35/50    Loss: 3.844494725227356\n",
      "\tEpoch:   35/50    Loss: 3.8844275913238526\n",
      "\tEpoch:   35/50    Loss: 3.8798460216522215\n",
      "\tEpoch:   35/50    Loss: 3.893621739387512\n",
      "\tEpoch:   35/50    Loss: 3.928049454212189\n",
      "\tEpoch:   35/50    Loss: 3.9284235525131224\n",
      "\tEpoch:   35/50    Loss: 3.954002670288086\n",
      "\tEpoch:   35/50    Loss: 3.9752287917137146\n",
      "\tEpoch:   35/50    Loss: 3.986572966098785\n",
      "Epoch 36\n",
      "\tEpoch:   36/50    Loss: 3.8332868409356036\n",
      "\tEpoch:   36/50    Loss: 3.796129695892334\n",
      "\tEpoch:   36/50    Loss: 3.8173770146369934\n",
      "\tEpoch:   36/50    Loss: 3.854312827587128\n",
      "\tEpoch:   36/50    Loss: 3.8579810638427734\n",
      "\tEpoch:   36/50    Loss: 3.8708926644325254\n",
      "\tEpoch:   36/50    Loss: 3.885235814571381\n",
      "\tEpoch:   36/50    Loss: 3.9121887798309327\n",
      "\tEpoch:   36/50    Loss: 3.9276032137870787\n",
      "\tEpoch:   36/50    Loss: 3.946643542766571\n",
      "\tEpoch:   36/50    Loss: 3.9572847228050234\n",
      "\tEpoch:   36/50    Loss: 3.974239848136902\n",
      "Epoch 37\n",
      "\tEpoch:   37/50    Loss: 3.84403020591789\n",
      "\tEpoch:   37/50    Loss: 3.770725977897644\n",
      "\tEpoch:   37/50    Loss: 3.7976949524879458\n",
      "\tEpoch:   37/50    Loss: 3.8154341621398924\n",
      "\tEpoch:   37/50    Loss: 3.8427899146080016\n",
      "\tEpoch:   37/50    Loss: 3.8783982815742495\n",
      "\tEpoch:   37/50    Loss: 3.892453677654266\n",
      "\tEpoch:   37/50    Loss: 3.9102026262283327\n",
      "\tEpoch:   37/50    Loss: 3.920982660770416\n",
      "\tEpoch:   37/50    Loss: 3.9320773253440855\n",
      "\tEpoch:   37/50    Loss: 3.9534599494934084\n",
      "\tEpoch:   37/50    Loss: 3.966814552783966\n",
      "Epoch 38\n",
      "\tEpoch:   38/50    Loss: 3.823950208993343\n",
      "\tEpoch:   38/50    Loss: 3.7688593473434446\n",
      "\tEpoch:   38/50    Loss: 3.8042438325881958\n",
      "\tEpoch:   38/50    Loss: 3.8245827651023863\n",
      "\tEpoch:   38/50    Loss: 3.8448973875045778\n",
      "\tEpoch:   38/50    Loss: 3.866051336765289\n",
      "\tEpoch:   38/50    Loss: 3.866679497241974\n",
      "\tEpoch:   38/50    Loss: 3.901032017707825\n",
      "\tEpoch:   38/50    Loss: 3.9064693965911865\n",
      "\tEpoch:   38/50    Loss: 3.9182057094573977\n",
      "\tEpoch:   38/50    Loss: 3.9379785299301147\n",
      "\tEpoch:   38/50    Loss: 3.949929323196411\n",
      "Epoch 39\n",
      "\tEpoch:   39/50    Loss: 3.805679625787443\n",
      "\tEpoch:   39/50    Loss: 3.7657399740219115\n",
      "\tEpoch:   39/50    Loss: 3.7786378064155577\n",
      "\tEpoch:   39/50    Loss: 3.805350960731506\n",
      "\tEpoch:   39/50    Loss: 3.8283199458122255\n",
      "\tEpoch:   39/50    Loss: 3.864761995315552\n",
      "\tEpoch:   39/50    Loss: 3.8671682848930358\n",
      "\tEpoch:   39/50    Loss: 3.8989159846305848\n",
      "\tEpoch:   39/50    Loss: 3.898830066204071\n",
      "\tEpoch:   39/50    Loss: 3.9268434166908266\n",
      "\tEpoch:   39/50    Loss: 3.9189232521057127\n",
      "\tEpoch:   39/50    Loss: 3.9412409973144533\n",
      "Epoch 40\n",
      "\tEpoch:   40/50    Loss: 3.800421266834716\n",
      "\tEpoch:   40/50    Loss: 3.755590645313263\n",
      "\tEpoch:   40/50    Loss: 3.7697033896446226\n",
      "\tEpoch:   40/50    Loss: 3.7989270730018614\n",
      "\tEpoch:   40/50    Loss: 3.816886100769043\n",
      "\tEpoch:   40/50    Loss: 3.846098794937134\n",
      "\tEpoch:   40/50    Loss: 3.872233423233032\n",
      "\tEpoch:   40/50    Loss: 3.882714903354645\n",
      "\tEpoch:   40/50    Loss: 3.8896321759223937\n",
      "\tEpoch:   40/50    Loss: 3.9102623581886293\n",
      "\tEpoch:   40/50    Loss: 3.9248611392974855\n",
      "\tEpoch:   40/50    Loss: 3.92617777299881\n",
      "Epoch 41\n",
      "\tEpoch:   41/50    Loss: 3.7871791844912557\n",
      "\tEpoch:   41/50    Loss: 3.7506204113960266\n",
      "\tEpoch:   41/50    Loss: 3.7945385994911196\n",
      "\tEpoch:   41/50    Loss: 3.813066505432129\n",
      "\tEpoch:   41/50    Loss: 3.7960344157218935\n",
      "\tEpoch:   41/50    Loss: 3.8351971130371094\n",
      "\tEpoch:   41/50    Loss: 3.8382356762886047\n",
      "\tEpoch:   41/50    Loss: 3.8510364899635317\n",
      "\tEpoch:   41/50    Loss: 3.8981255502700805\n",
      "\tEpoch:   41/50    Loss: 3.9062458567619323\n",
      "\tEpoch:   41/50    Loss: 3.9074423298835756\n",
      "\tEpoch:   41/50    Loss: 3.9125519399642945\n",
      "Epoch 42\n",
      "\tEpoch:   42/50    Loss: 3.791495401547148\n",
      "\tEpoch:   42/50    Loss: 3.75315362071991\n",
      "\tEpoch:   42/50    Loss: 3.7657979660034178\n",
      "\tEpoch:   42/50    Loss: 3.775250963687897\n",
      "\tEpoch:   42/50    Loss: 3.804330380439758\n",
      "\tEpoch:   42/50    Loss: 3.8381602244377135\n",
      "\tEpoch:   42/50    Loss: 3.836761803150177\n",
      "\tEpoch:   42/50    Loss: 3.864006319999695\n",
      "\tEpoch:   42/50    Loss: 3.8713448939323425\n",
      "\tEpoch:   42/50    Loss: 3.9080138297080995\n",
      "\tEpoch:   42/50    Loss: 3.9103019571304323\n",
      "\tEpoch:   42/50    Loss: 3.902264276504517\n",
      "Epoch 43\n",
      "\tEpoch:   43/50    Loss: 3.76156035273188\n",
      "\tEpoch:   43/50    Loss: 3.7271426696777343\n",
      "\tEpoch:   43/50    Loss: 3.7532101860046385\n",
      "\tEpoch:   43/50    Loss: 3.782685329437256\n",
      "\tEpoch:   43/50    Loss: 3.7902254338264467\n",
      "\tEpoch:   43/50    Loss: 3.792547015666962\n",
      "\tEpoch:   43/50    Loss: 3.8235322484970093\n",
      "\tEpoch:   43/50    Loss: 3.856456689834595\n",
      "\tEpoch:   43/50    Loss: 3.873620581150055\n",
      "\tEpoch:   43/50    Loss: 3.8709423031806947\n",
      "\tEpoch:   43/50    Loss: 3.9020668992996215\n",
      "\tEpoch:   43/50    Loss: 3.92716738986969\n",
      "Epoch 44\n",
      "\tEpoch:   44/50    Loss: 3.7533485155583755\n",
      "\tEpoch:   44/50    Loss: 3.720932322502136\n",
      "\tEpoch:   44/50    Loss: 3.7341185555458067\n",
      "\tEpoch:   44/50    Loss: 3.770304750442505\n",
      "\tEpoch:   44/50    Loss: 3.795112070083618\n",
      "\tEpoch:   44/50    Loss: 3.827870180130005\n",
      "\tEpoch:   44/50    Loss: 3.816090572834015\n",
      "\tEpoch:   44/50    Loss: 3.843310820579529\n",
      "\tEpoch:   44/50    Loss: 3.876058020591736\n",
      "\tEpoch:   44/50    Loss: 3.86825594329834\n",
      "\tEpoch:   44/50    Loss: 3.877622829437256\n",
      "\tEpoch:   44/50    Loss: 3.9079665088653566\n",
      "Epoch 45\n",
      "\tEpoch:   45/50    Loss: 3.7358490464415057\n",
      "\tEpoch:   45/50    Loss: 3.7140689067840578\n",
      "\tEpoch:   45/50    Loss: 3.731225709915161\n",
      "\tEpoch:   45/50    Loss: 3.7621889243125914\n",
      "\tEpoch:   45/50    Loss: 3.784494462490082\n",
      "\tEpoch:   45/50    Loss: 3.8136972832679747\n",
      "\tEpoch:   45/50    Loss: 3.8180628533363343\n",
      "\tEpoch:   45/50    Loss: 3.843586597442627\n",
      "\tEpoch:   45/50    Loss: 3.8536715726852417\n",
      "\tEpoch:   45/50    Loss: 3.8441919384002685\n",
      "\tEpoch:   45/50    Loss: 3.858751829624176\n",
      "\tEpoch:   45/50    Loss: 3.901159739494324\n",
      "Epoch 46\n",
      "\tEpoch:   46/50    Loss: 3.755141230346765\n",
      "\tEpoch:   46/50    Loss: 3.707580816268921\n",
      "\tEpoch:   46/50    Loss: 3.73560915517807\n",
      "\tEpoch:   46/50    Loss: 3.7511565618515013\n",
      "\tEpoch:   46/50    Loss: 3.7726970391273498\n",
      "\tEpoch:   46/50    Loss: 3.8016623101234437\n",
      "\tEpoch:   46/50    Loss: 3.815742787837982\n",
      "\tEpoch:   46/50    Loss: 3.823202484130859\n",
      "\tEpoch:   46/50    Loss: 3.846059476852417\n",
      "\tEpoch:   46/50    Loss: 3.8629119391441344\n",
      "\tEpoch:   46/50    Loss: 3.891451494216919\n",
      "\tEpoch:   46/50    Loss: 3.8875301938056945\n",
      "Epoch 47\n",
      "\tEpoch:   47/50    Loss: 3.740091662885087\n",
      "\tEpoch:   47/50    Loss: 3.698673316001892\n",
      "\tEpoch:   47/50    Loss: 3.721798885345459\n",
      "\tEpoch:   47/50    Loss: 3.754124617099762\n",
      "\tEpoch:   47/50    Loss: 3.7602915725708006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch:   47/50    Loss: 3.7926476306915284\n",
      "\tEpoch:   47/50    Loss: 3.8100808572769167\n",
      "\tEpoch:   47/50    Loss: 3.8128470487594606\n",
      "\tEpoch:   47/50    Loss: 3.8279855909347535\n",
      "\tEpoch:   47/50    Loss: 3.840764820098877\n",
      "\tEpoch:   47/50    Loss: 3.8656172556877135\n",
      "\tEpoch:   47/50    Loss: 3.8770635418891906\n",
      "Epoch 48\n",
      "\tEpoch:   48/50    Loss: 3.7297622184567465\n",
      "\tEpoch:   48/50    Loss: 3.6957771711349485\n",
      "\tEpoch:   48/50    Loss: 3.7188016238212587\n",
      "\tEpoch:   48/50    Loss: 3.743734484195709\n",
      "\tEpoch:   48/50    Loss: 3.7576294927597047\n",
      "\tEpoch:   48/50    Loss: 3.7890750665664674\n",
      "\tEpoch:   48/50    Loss: 3.7982600884437563\n",
      "\tEpoch:   48/50    Loss: 3.8051732048988343\n",
      "\tEpoch:   48/50    Loss: 3.8262466049194335\n",
      "\tEpoch:   48/50    Loss: 3.8372292265892027\n",
      "\tEpoch:   48/50    Loss: 3.871657735347748\n",
      "\tEpoch:   48/50    Loss: 3.8890133805274965\n",
      "Epoch 49\n",
      "\tEpoch:   49/50    Loss: 3.730587804549916\n",
      "\tEpoch:   49/50    Loss: 3.6714827566146853\n",
      "\tEpoch:   49/50    Loss: 3.7045059514045717\n",
      "\tEpoch:   49/50    Loss: 3.7387246646881103\n",
      "\tEpoch:   49/50    Loss: 3.732474600791931\n",
      "\tEpoch:   49/50    Loss: 3.7606515102386475\n",
      "\tEpoch:   49/50    Loss: 3.8013355021476745\n",
      "\tEpoch:   49/50    Loss: 3.824206744670868\n",
      "\tEpoch:   49/50    Loss: 3.833846056461334\n",
      "\tEpoch:   49/50    Loss: 3.827072145462036\n",
      "\tEpoch:   49/50    Loss: 3.8827694215774535\n",
      "\tEpoch:   49/50    Loss: 3.8588559937477114\n",
      "Epoch 50\n",
      "\tEpoch:   50/50    Loss: 3.7124845417097085\n",
      "\tEpoch:   50/50    Loss: 3.6866064291000367\n",
      "\tEpoch:   50/50    Loss: 3.710351957798004\n",
      "\tEpoch:   50/50    Loss: 3.731842719078064\n",
      "\tEpoch:   50/50    Loss: 3.7407905478477477\n",
      "\tEpoch:   50/50    Loss: 3.7713396701812743\n",
      "\tEpoch:   50/50    Loss: 3.777004051208496\n",
      "\tEpoch:   50/50    Loss: 3.810281939983368\n",
      "\tEpoch:   50/50    Loss: 3.8121349816322327\n",
      "\tEpoch:   50/50    Loss: 3.829251958847046\n",
      "\tEpoch:   50/50    Loss: 3.846245280265808\n",
      "\tEpoch:   50/50    Loss: 3.862978262901306\n"
     ]
    }
   ],
   "source": [
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn, loss = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14196\\Anaconda3\\envs\\ml37\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# saving the trained model\n",
    "save_model('./save/trained_rnn_trimmed', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x229193a6fc8>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning (3.7)",
   "language": "python",
   "name": "ml37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
