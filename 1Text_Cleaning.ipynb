{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent\n",
    "The purpose of this notebook is to take raw text downloaded from a [website](https://www.hplovecraft.com/writings/texts/). We will remove punctuation, unnecessary white space, convert to lowercase, and otherwise standardize all text files into a single text file for processing in a ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory(data_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Load all text files in the specified directory. The contents of these files are then concatenated into one, long string.\n",
    "    \n",
    "    :param data_dir: The data directory containing the text files we want to load.\n",
    "    :return all_text_data: The concatenated text data of the files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Relevant files\n",
    "    text_files = [\n",
    "        text_file for text_file in os.listdir(data_dir) \n",
    "        if text_file.endswith('.txt')\n",
    "    ]\n",
    "    \n",
    "    # Combine all text data into one string\n",
    "    all_text_data = \" \".join(\n",
    "        [\n",
    "            load_data(os.path.join(data_dir, text_file))\n",
    "            for text_file in text_files\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {len(text_files)} text files from {data_dir}\")\n",
    "    return all_text_data\n",
    "\n",
    "\n",
    "def load_data(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Open and read data from specified file.\n",
    "    \n",
    "    :param path: Path to the file.\n",
    "    :return data: The contents of the file.\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "\n",
    "def token_lookup() -> dict:\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into tokens.\n",
    "    \n",
    "    :return tokens: Tokenized dictionary where the key is the punctuation and the value is the converted token\n",
    "    \"\"\"\n",
    "    tokens = dict()\n",
    "    tokens['.'] = ' <PERIOD> '\n",
    "    tokens['!'] = ' <EXCLAMATION_MARK> '\n",
    "    tokens['?'] = ' <QUESTION_MARK> '\n",
    "    tokens[','] = ' <COMMA> '\n",
    "    tokens['\"'] = ' <QUOTATION_MARK> '\n",
    "    tokens[';'] = ' <SEMICOLON> '\n",
    "    tokens['('] = ' <LEFT_PAREN> '\n",
    "    tokens[')'] = ' <RIGHT_PAREN> '\n",
    "    tokens['-'] = ' <DASH> '\n",
    "    tokens['â€”'] = ' <DASH> '\n",
    "    tokens['\\n'] = ' <NEW_LINE> '\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def clean_text(text: str, special_tokens_dict: dict) -> str:\n",
    "    # Iterate through all tokens we want to remove\n",
    "    for k, v in special_tokens_dict.items():\n",
    "        text = text.replace(k, v)\n",
    "        \n",
    "    # Reduce white space to single space\n",
    "    text = ' '.join(text.split())\n",
    "    # Reduce to lower case to reduce complexity of words\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def filter_text(text):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    word_counter = Counter(tokens)\n",
    "    trimmed_words = [w for w, cnt in word_counter.items() if cnt > 3]\n",
    "    trimmed_tokens = [t for t in tokens if t in trimmed_words]\n",
    "    return trimmed_tokens\n",
    "\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # Counter for all words\n",
    "    word_counter = Counter(text)\n",
    "    # Need to make sure that our filler text is included with our vocabulary\n",
    "    word_counter['<PAD>'] += 1\n",
    "    \n",
    "    # Sort words by frequency\n",
    "    sorted_words = sorted(word_counter, key=word_counter.get, reverse=True)\n",
    "    \n",
    "    # Create int_to_vocab, vocab_to_int\n",
    "    # We are filtering out infrequent tokens, as well as the <PAD> token\n",
    "    int_to_vocab = {ii:word for ii, word in enumerate(sorted_words)}\n",
    "    vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
    "    \n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(data_dir, output_file='preprocess.dat'):\n",
    "    \"\"\"\n",
    "    Preprocess Text Data\n",
    "    \"\"\"\n",
    "    all_text_data = load_directory(data_dir)\n",
    "    special_tokens_dict = token_lookup()\n",
    "    cleaned_text = clean_text(all_text_data, special_tokens_dict)\n",
    "    tokens = filter_text(cleaned_text)\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(tokens)\n",
    "    int_text = [vocab_to_int[word] for word in tokens]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, special_tokens_dict), open(output_file, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory is: C:\\Users\\14196\\Documents\\Projects\\Machine_Learning_Projects\\NLP_Projects\\lovecraft_text_generator\\data\n",
      "Loaded 105 text files from C:\\Users\\14196\\Documents\\Projects\\Machine_Learning_Projects\\NLP_Projects\\lovecraft_text_generator\\data\n"
     ]
    }
   ],
   "source": [
    "# Relevant directories\n",
    "current_dir = os.path.abspath('')\n",
    "data_dir = os.path.join(current_dir, \"data\")\n",
    "print(f\"Data directory is: {data_dir}\")\n",
    "\n",
    "preprocess_and_save_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications\n",
    "We are struggling to get below a training Cross Entropy Loss of 4.5. Resources indicate that to achieve a more realistic output, a loss of 3.5 or lower is encouraged. \n",
    "\n",
    "Therefore, to assist with improving performance, the below analysis was performed to determine that tokens occurring less than 3 times consituted a total of 1.79% of all tokens in the text, and showed a drastic increase of occurrance from tokens occurring only once (1.28%). <More here on why this value was selected; if results are promising>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nall_text_data = load_directory(data_dir)\\nspecial_tokens_dict = token_lookup()\\ncleaned_text = clean_text(all_text_data, special_tokens_dict)\\ntokens = cleaned_text.split(\" \")\\ntokens.append(\\'<PAD>\\')\\nvocab_to_int, int_to_vocab = create_lookup_tables(tokens)\\nint_text = [vocab_to_int[word] for word in vocab_to_int]\\n\\n\\nword_counter = Counter(tokens)\\n\\nfor token_freq in range(1,6):\\n    infrequent_tokens = [(token, word_counter[token]) for token in word_counter if word_counter[token] <= token_freq]\\n    per = len(infrequent_tokens)/len(tokens)\\n    print(f\"{len(infrequent_tokens)} tokens occur at or less than {token_freq} time{\\'s\\' if token_freq > 1 else \\'\\'}.\")\\n    if token_freq > 1:\\n          it_prev = [(token, word_counter[token]) for token in word_counter if word_counter[token] <= (token_freq - 1)]\\n          per_change = (len(infrequent_tokens) - len(it_prev)) / len(it_prev) * 100\\n          print(f\"A {per_change:.2f}% change from the previous iteration.\")\\n    print(f\"This is {per*100:.2f}% of the total tokens.\\n\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "all_text_data = load_directory(data_dir)\n",
    "special_tokens_dict = token_lookup()\n",
    "cleaned_text = clean_text(all_text_data, special_tokens_dict)\n",
    "tokens = cleaned_text.split(\" \")\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(tokens)\n",
    "int_text = [vocab_to_int[word] for word in vocab_to_int]\n",
    "\n",
    "\n",
    "word_counter = Counter(tokens)\n",
    "\n",
    "for token_freq in range(1,6):\n",
    "    infrequent_tokens = [(token, word_counter[token]) for token in word_counter if word_counter[token] <= token_freq]\n",
    "    per = len(infrequent_tokens)/len(tokens)\n",
    "    print(f\"{len(infrequent_tokens)} tokens occur at or less than {token_freq} time{'s' if token_freq > 1 else ''}.\")\n",
    "    if token_freq > 1:\n",
    "          it_prev = [(token, word_counter[token]) for token in word_counter if word_counter[token] <= (token_freq - 1)]\n",
    "          per_change = (len(infrequent_tokens) - len(it_prev)) / len(it_prev) * 100\n",
    "          print(f\"A {per_change:.2f}% change from the previous iteration.\")\n",
    "    print(f\"This is {per*100:.2f}% of the total tokens.\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning (3.6)",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
